 ---
 Distributed and Replicated Caching
 ---


Distributed and Replicated Caching

     Many production applications are deployed in clusters of multiple
     instances for availability and scalability.  However, without a
     distributed or replicated cache, application
     clusters exhibit a number of undesirable behaviors, such as:

     * <<Cache Drift>>--if each application instance maintains its own cache,
       updates made to one cache will not appear in the other instances.  This
       also happens to web session data.  A distributed or replicated cache ensures that all
       of the cache instances are kept in sync with each other.
     
     * <<Database Bottlenecks>>--In a single-instance
       application, a cache effectively shields a database from the overhead of
       redundant queries.  However, in a distributed application environment,
       each instance much load and keep its own cache fresh.  The overhead of
       loading and refreshing multiple caches leads to database bottlenecks as
       more application instances are added.  A distributed or replicated cache eliminates
       the per-instance overhead of loading and refreshing multiple caches from
       a database.

     []

* Distributed Caching

     Ehcache comes bundled with a distributed caching mechanism using Terracotta that enables
     multiple CacheManagers and their caches in multiple JVMs to share data
     with each other.  Adding distributed caching to Ehcache takes only two lines of configuration.

     Using Terracotta for Ehcache distributed caching is the recommended
     method of operating Ehcache in a distributed or scaled-out application
     environment.  It provides the highest level of performance, availability,
     and scalability.  As the maintainers of Ehcache, the Terracotta development
     team has invested million of hours developing Ehcache and its distributed
     cache capabilities.

     To get started, see the {{{./terracotta/distributed_caching_with_terracotta.html}Distributed Caching With Terracotta chapter}}.

* Replicated Caching

    In addition to the built-in distributed caching, Ehcache has a pluggable cache replication scheme which enables the addition of cache replication mechanisms.

    The following additional replicated caching mechanisms are available:

    * RMI

    * JGroups

    * JMS

    * Cache Server

   Each of the is covered in its own chapter.

     One solution is to replicate data between the caches to keep them
     consistent, or coherent. Typical operations which Applicable
     operations include:

       * put

       * update (put which overwrites an existing entry)

       * remove

     Update supports updateViaCopy or updateViaInvalidate. The latter
      sends the a remove message out to the cache cluster, so that other
      caches remove the Element, thus preserving coherency. It is typically
      a lower cost option than a copy.


** Using a Cache Server

    Ehcache 1.5 supports the Ehcache Cache Server.

    To achieve shared data, all JVMs read to and write from a Cache Server, which runs
    it in its own JVM.

    To achieve redundancy, the Ehcache inside the Cache Server can be set up in its own cluster.

    This technique will be expanded upon in Ehcache 1.6.


** Notification Strategies

     The best way of notifying of put and update depends on the nature of
     the cache.

     If the Element is not available anywhere else then the Element
     itself should form the payload of the notification. An example is a
     cached web page. This notification strategy is called copy.

     Where the cached data is available in a database, there are two
     choices. Copy as before, or invalidate the data. By invalidating the
     data, the application tied to the other cache instance will be forced
     to refresh its cache from the database, preserving cache coherency.
     Only the Element key needs to be passed over the network.

     Ehcache supports notification through copy and invalidate, selectable per cache.

**   Potential Issues with Replicated Caching

*** Potential for Inconsistent Data

     Timing scenarios, race conditions, delivery, reliability
     constraints and concurrent updates to the same cached data can cause
     inconsistency (and thus a lack of coherency) across the cache
     instances.

     This potential exists within the Ehcache implementation.
     These issues are the same as what is seen when two completely
     separate systems are sharing a database; a common scenario.

     Whether data inconsistency is a problem depends on the data and how it
     is used. For those times when it is important, Ehcache provides for
     synchronous delivery of puts and updates via invalidation. These are discussed below:

**** Synchronous Delivery

     Delivery can be specified to be synchronous or asynchronous.
     Asynchronous delivery gives faster returns to operations on the local
     cache and is usually preferred. Synchronous delivery adds time to the
     local operation, however delivery of an update to
     all peers in the cluster happens before the cache operation returns.


**** Put and Update via Invalidation

     The default is to update other caches by copying the new value to
     them. If the replicatePutsViaCopy property is set to false in the
     replication configuration, puts are made by removing the element in
     any other cache peers. If the replicateUpdatesViaCopy property is set to false in the
     replication configuration, updates are made by removing the element in
     any other cache peers.

     This forces the applications using the cache
     peers to return to a canonical source for the data.

     A similar effect can be obtained by setting the element TTL to a low value such
     as a second.

     Note that these features impact cache performance and should
     not be used where the main purpose of a cache is performance boosting over
     coherency.


***  Use of Time To Idle

     Time To Idle is inconsistent with replicated caching. Time-to-idle makes some entries live longer on some
     nodes than in others because of cache usage patterns. However, the cache entry "last touched" timestamp
     is not replicated across the distributed cache.

     Do not use Time To Idle with distributed caching, unless you do not care about inconsistent data across nodes. 



     
