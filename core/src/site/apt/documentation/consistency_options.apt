 ---
 Cache Consistency Options
 ---

Consistency Options

~~%{toc|fromDepth=2|toDepth=3}


    The purpose of this chapter is firstly to explain Distributed Ehcache's consistency modes in terms of the standard
    nomenclature on client side consistency, to show how we achieve various consistencies in the distributed cache topology
    and finally to provide some general advice on when various modes should be used.

*   The Standard Client-Side Consistency Model

   Werner Vogel's seminal Eventually Consistent paper presented standard terms for client-side consistency and a way
   of reasoning about whether that consistency can be achieved in a distributed system. This paper in turn
   referenced Tannenbaum's {{{http://www.amazon.com/Distributed-Systems-Principles-Paradigms-2nd/dp/0132392275/ref=dp_ob_title_bk}Distributed Systems: Principles and Paradigms (2nd Edition)}}.

   Before explaining our consistency modes, we need to expain the standard components of the the reference model
   which is an abstract model of a distributed system that can be used for studying interactions.

    * A storage system. The storage system consists of data stored durably in one server or multiple servers connected
      via a network. In Ehcache durability is optional and the storage system might simply be in memory.

    * Client Process A. ?This is a process that writes to and reads from the storage system.

    * Client Processes B and C. ?These two processes are independent of process A and write to and read from the
      storage system. It is irrelevant whether these are really processes or threads within the same process;
      what is important is that they are independent and need to communicate to share information.
      ?Client-side consistency has to do with how and when observers (in this case the processes A, B, or C)
      see updates made to a data object in the storage systems.

      It then goes on to define the following consistencies where process A has made an update to a data object:

    * Strong consistency. After the update completes, any subsequent access (by A, B, or C) will return the updated value.

    * Weak consistency. The system does not guarantee that subsequent accesses will return the updated value.

    * Eventual consistency. This is a specific form of weak consistency; the storage system guarantees that if no new
      updates are made to the object, eventually all accesses will return the last updated value. If no failures occur,
      the maximum size of the inconsistency window can be determined based on factors such as communication delays,
      the load on the system, and the number of replicas involved in the replication scheme.

    Within eventual consistency there are a number of desirable properties:

    * Read-your-writes consistency. This is an important model where process A, after it has updated a data item,
      always accesses the updated value and will never see an older value. This is a special case of the causal consistency
      model.

    * Session consistency. This is a practical version of the previous model, where a process accesses the storage system
      in the context of a session. As long as the session exists, the system guarantees read-your-writes consistency.
      If the session terminates because of a certain failure scenario, a new session needs to be created and the guarantees
      do not overlap the sessions.

    * Monotonic read consistency. If a process has seen a particular value for the object, any subsequent accesses will never
      return any previous values.

    * Monotonic write consistency?. In this case the system guarantees to serialize the writes by the same process.
      Systems that do not guarantee this level of consistency are notoriously hard to program.

    Finally, in eventual consistency, the period between the update and the moment when it is
      guaranteed that any observer will always see the updated value is dubbed the inconsistency window.

*   Ehcache Topologies

    Ehcache is available with the following clustered caching topologies:

    * Standalone - the cached data set is held in the application node. Any other application nodes are independent with no communication
      between them. If standalone caching is being used where there are multiple application nodes running the same application, then
      there is Weak Consistency between them. Indeed they will only reflect the same values for immutable data or after the time to live
      on an Element has completed and the Element needs to be reloaded.

    * Replicated - the cached data set is held in each application node and data is copied or invalidated across the cluster without locking.
      Replication can be either asynchronous or synchronous, where the writing thread blocks while progagation occurs. The only
      consistency mode available in this topology is Weak Consistency.

    * Distributed - the data is held in a Terracotta Server Array ("SA") with a subset of recently used data held in each application cache node.
      Writes may be either synchronous or asynchronous.


      The distributed topology supports a very rich set of consistency modes which will be explored in this chapter.

*   Proof of Distributed Ehcache’s Strong Consistency

    In consistency=strong mode, this proof will show that an update which in Ehcache terms is a cache.put() or cache.remove() operation is actually strongly coherent as defined.
    This analysis assumes that if NonStopCache is being used, it is configured with the default of Exception, so that on a clusterOffline event no cache operations happen locally.
    For convenience, the Terracotta terminology is mapped to the reference model as follows:
        •       Ehcache A = Process A
        •       Ehcache B = Process B
        •       Ehcache C = Process C
        •       Terracotta Server Array = Storage System
        •       Write = Update
        1.      A thread in Ehcache A performs a write.
        2.      Before the write is done, a write lock is obtained from the Terracotta Server (storage system). The write lock is granted only after all read locks have been surrendered.
        3.      The write is done to an in-process Transaction Buffer. Within the Java process the write is thread-safe. Any local threads in Ehcache A will have immediate visibility of the change.
        4.      Once the change has hit the Transaction Buffer which is a LinkedBlockingQueue, a notify occurs, and the Transaction Buffer initiates sending the write (update) to the Terracotta Server Array (storage system). This write may be asynchronous (default) or synchronous (by configuration using synchronous=true).
        5.      The Terracotta Server is generally configured with multiple replicas forming a Mirror Group. Within the mirror group there is an Active server, and one or more Passive servers.  The write is to the Active server. The Active server does not acknowledge the write until it has written it to each of the passive servers in the Mirror Group. It then sends back an acknowledgement to Ehcache A which then deletes the write from the Transaction Buffer.
        6.      A read or write request from Ehcache A is immediately available because a read lock is automatically granted when a write lock has already been acquired. A read or write request in Ehcache B or C requires the acquisition of a read or write lock respectively which will block until step 5 has occurred, and in addition, if you have a stale copy locally it is updated first. When the lock is granted the write is present in all replicas. Because Ehcache also maintains copies of Elements in-process in potentially each node, if any of Ehcache A, B or C have a copy they are also updated before 5 completes.




