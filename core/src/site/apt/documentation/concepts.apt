 ---
 Key Concepts
 ---

{Key Ehcache Concepts}

* Definitions

    * <<{cache-hit}>>:
    When a data element is requested of the cache and the element exists for 
    the given key, it is referrred to as a cache hit (or simply 'hit').

    * <<{cache-miss}>>:
    When a data element is requested of the cache and the element does not 
    exist for the given key, it is referred to as a cache miss (or simply
    'miss').

    * <<{system-of-record}>>: 
    The core premise of caching assumes that there is a source of truth for 
    the data.  This is often referred to as a 
    {{{http://en.wikipedia.org/wiki/System_of_record}system-of-record (SOR)}}.
    The cache acts as a local copy of data retrieved from or stored to 
    the system-of-record.  

    * <<{SOR}>>:
    See {{system-of-record}}.

* Key Ehcache Classes

[javadoc/net/sf/ehcache/package.png] Top Level Package Diagram

    Ehcache consists of a <<<CacheManager>>>, which manages caches. Caches contain elements,
    which are essentially name value pairs. Caches are physically implemented either in-memory, or on disk.

** {CacheManager}

[javadoc/net/sf/ehcache/CacheManager.png] CacheManager Class Diagram

    The <<<CacheManager>>> comprises Caches which in turn comprise Elements.

    Creation of, access to and removal of caches is controlled by the <<<CacheManager>>>.

*** CacheManager Creation Modes

   <<<CacheManager>>> supports two creation modes: singleton and instance.

**** Singleton Mode

    Ehcache-1.1 supported only one <<<CacheManager>>>
    instance which was a singleton. CacheManager can still be used in this
    way using the static factory methods.

**** Instance Mode

    From ehcache-1.2, CacheManager has constructors which mirror the
    various static create methods. This enables multiple CacheManagers to
    be created and used concurrently. Each CacheManager requires its own
    configuration.

    If the Caches under management use only the MemoryStore, there
    are no special considerations. If Caches use the DiskStore, the
    diskStore path specified in each CacheManager configuration should be
    unique. When a new CacheManager is created, a check is made that there
    are no other CacheManagers using the same diskStore path. If there are,
    a CacheException is thrown. If a CacheManager is part of a cluster,
    there will also be listener ports which must be unique.

**** Mixed Singleton and Instance Mode

    If an application creates instances of CacheManager using a
    constructor, and also calls a static create method, there will exist a
    singleton instance of CacheManager which will be returned each time the
    create method is called together with any other instances created via
    constructor. The two types will coexist peacefully.



** Ehcache

[javadoc/net/sf/ehcache/Ehcache.png] Ehcache Interface Diagram

    All caches implement the <<<Ehcache>>> interface. A cache has a name and attributes. Each cache contains Elements.

    A Cache in Ehcache is analogous to a cache region in other caching systems.

    Cache elements are stored in the <<<MemoryStore>>>. Optionally they also overflow to a <<<DiskStore>>>.

** {Element}

[javadoc/net/sf/ehcache/Element.png] Element Class Diagram

    An element is an atomic entry in a cache. It has a key, a value and a record of
    accesses. Elements are put into and removed from caches. They can also
    expire and be removed by the Cache, depending on the Cache settings.

    As of ehcache-1.2 there is an API for Objects in addition to the one for Serializable. Non-serializable Objects can
    use all parts of Ehcache except for DiskStore and replication. If an attempt is made to persist or replicate them
    they are discarded without error and with a DEBUG level log message.

    The APIs are identical except for the return methods from Element. Two new methods on
    Element: getObjectValue and getKeyValue are the only API differences between the Serializable and Object APIs. This
    makes it very easy to start with caching Objects and then change your Objects to Seralizable to participate in
    the extra features when needed. Also a large number of Java classes are simply not Serializable.

* Cache Usage Patterns

    There are several common access patterns when using a cache.  Ehcache 
    supports the following patterns:

    * {{cache-aside}} (or direct manipulation)

    * {{cache-as-sor}} (a combination of read-through and write-through or write-behind patterns)

    * {{read-through}}

    * {{write-through}}

    * {{write-behind}} (or write-back)
    []

** cache-aside

    Here, application code uses the cache directly.  

    This means that application code which accesses the {{system-of-record}}
    (SOR) should consult the cache first, and if the cache contains the data, 
    then return the data directly from the cache, bypassing the SOR.  

    Otherwise, the application code must fetch the data from the 
    system-of-record, store the data in the cache, and then return it.

    When data is written, the cache must be updated with the system-of-record.

    This results in code that often looks like the following pseudo-code:

------------------------
public class MyDataAccessClass 
{
    private final Ehcache cache;

    public MyDataAccessClass(Ehcache cache)
    {
        this.cache = cache;
    }
    
    /* read some data, check cache first, otherwise read from sor */
    public V readSomeData(K key) 
    {
        Element element;
        if ((element = cache.get(key)) != null) {
            return element.getValue();
        }

        // note here you should decide whether your cache
        // will cache 'nulls' or not
        if (value = readDataFromDataStore(key)) != null) {
            cache.put(new Element(key, value));
        } 

        return value;
    }


    /* write some data, write to sor, then update cache */
    public void writeSomeData(K key, V value) 
    {
        writeDataToDataStore(key, value);
        cache.put(new Element(key, value);
    }
}
------------------------

** cache-as-sor

    The cache-as-sor pattern implies using the cache as though it
    were the primary {{system-of-record}} (SOR).  The pattern delegates SOR 
    reading and writing activies to the cache, so that application
    code is absolved of this responsibility.

    To implement the cache-as-sor pattern, use a combination of the 
    following read and write patterns:

    * read-through 

    * write-through or write-behind

    []

    Advantages of using the cache-as-sor pattern are:
    
    * less cluttered application code (improved maintainability)
    
    * easily choose between write-through or write-behind strategies on a 
    per-cache basis (use only configuration)

    * allow the cache to solve the "thundering-herd" problem

    []

    Disadvantages are:

    * less directly visible code-path

    []

** read-through

    The read-through pattern mimics the structure of the cache-aside pattern
    when reading data.  The difference is that you must implement the 
    <<<CacheEntryFactory>>> interface to instruct the cache how to read 
    objects on a cache miss, and you must wrap the Ehcache instance with
    an instance of <<<SelfPopulatingCache>>>.

    Compare the appearance of the read-through pattern code to the 
    code provided in the cache-aside pattern.  (The full example is 
    provided at the end of this document that includes a read-through 
    and write-through implementation).

** write-through

    The write-through pattern mimics the structure of the cache-aside pattern
    when writing data.  The difference is that you must implement the
    <<<CacheWriter>>> interface and configure the cache for write-through
    or write-behind.

    A write-through cache writes data to the system-of-record in the same
    thread of execution, therefore in the common scenario of using a database
    transaction in context of the thread, the write to the database is covered 
    by the transaction in scope.

    More details (including configuration settings) can be found in the User
    Guide chapter on 
    {{{./write_through_caching.html}Write-through and Write-behind Caching}}.

** write-behind

    The write-behind pattern changes the timing of the write to the 
    system-of-record. Rather than writing to the System of Record in the 
    same thread of execution, write-behind queues the data for write at a 
    later time.

    The consequences of the change from write-through to write-behind are that
    the data write using write-behind will occur outside of the scope of the 
    transaction.

    This often-times means that a new transaction must be created to commit
    the data to the system-of-record that is separate from the main transaction.
    
    More details (including configuration settings) can be found in the User
    Guide chapter on 
    {{{./write_through_caching.html}Write-through and Write-behind Caching}}.
 
** cache-as-sor example

------------------------
public class MyDataAccessClass 
{
    private final Ehcache cache;

    public MyDataAccessClass(Ehcache cache)
    {
        cache.registerCacheWriter(new MyCacheWriter());
        this.cache = new SelfPopulatingCache(cache);
    }

    /* read some data - notice the cache is treated as an SOR.  
     * the application code simply assumes the key will always be available
     */
    public V readSomeData(K key) 
    {
        return cache.get(key);
    }

    /* write some data - notice the cache is treated as an SOR, it is 
     * the cache's responsibility to write the data to the SOR. 
     */
    public void writeSomeData(K key, V value) 
    {
        cache.put(new Element(key, value);
    }

    /**
     * Implement the CacheEntryFactory that allows the cache to provide
     * the read-through strategy
     */
    private class MyCacheEntryFactory implements CacheEntryFactory
    {
        public Object createEntry(Object key) throws Exception
        {
            return readDataFromDataStore(key);
        }    
    }

    /**
     * Implement the CacheWriter interface which allows the cache to provide
     * the write-through or write-behind strategy.
     */
    private class MyCacheWriter implements CacheWriter 
        public CacheWriter clone(Ehcache cache) throws CloneNotSupportedException;
        {
            throw new CloneNotSupportedException();
        }

        public void init() { }
        void dispose() throws CacheException { } 

        void write(Element element) throws CacheException;
        {
            writeDataToDataStore(element.getKey(), element.getValue());
        }

        void writeAll(Collection<Element> elements) throws CacheException
        {
            for (Element element : elements) {
                write(element);
            }
        }

        void delete(CacheEntry entry) throws CacheException
        {
            deleteDataFromDataStore(element.getKey());
        }

        void deleteAll(Collection<CacheEntry> entries) throws CacheException
        {
            for (Element element : elements) {
                delete(element);
            }
        }
    }
}
------------------------

* Data access problems (and how to solve them)

    There are many problems which users may experience when accessing 
    data in their application.  Some of the more common issues, and 
    common solutions to these problems, are presented here.

** overloaded database (or other SOR) for frequently read data problem

    This data access problem is exactly why caching products exist.  The
    problem arises when many readers wish to read the same element of data.  

    This can happen for example when a popular news story hits the front
    page, or it can be the result of a common data element, such as the 
    use of a lookup table (e.g. U.S. states to abbreviations, zip codes, 
    etc.) is used in an application.

    In a typical unoptimized application that uses a database as an SOR
    to store the data element, the application would repeatedly read the
    same element of data from the database over and over and over again.

    This is not what databases should be used for!

    <<Solution>>

    The overloaded database problem can easily be solved by introducing a cache
    into the application - it is the main use case for a cache!

    When a data element is frequently used, it can be placed into the cache
    for easy access on subsequent reads so that the SOR is relieved of this duty.

** thundering herd problem

    The "thundering herd" problem occurs in a highly concurrent environment
    (typically, many users).  When many users make a request to the same 
    piece of data at the same time, and there is a cache miss (the data
    for the cached element is not present in the cache) the thundering 
    herd problem is triggered.

    Imagine that a popular news story has just surfaced on the front page
    of a news site.  The news story has not yet been loaded in to the 
    cache.

    The application is using a cache using a {{read-through}} pattern with
    code that looks approximately like:

----------------------------
101    /* read some data, check cache first, otherwise read from sor */
102    public V readSomeData(K key)
103    {
104        Element element;
105        if ((element = cache.get(key)) != null) {
106            return element.getValue();
107        }
108
109        // note here you should decide whether your cache
110        // will cache 'nulls' or not
111        if (value = readDataFromDataStore(key)) != null) {
112            cache.put(new Element(key, value));
113        }
114
115        return value;
116    }
----------------------------

    Upon publication to the front page of a website, a news story will
    then likely be clicked on by many users all at approximately the same
    time.

    Since the application server is processing all of the user requests
    simultaneously, the application code will execute the above code all
    at approximately the same time.  This is especially important to 
    consider, because all user requests will be evaluating the cache 
    (line 105) contents at approximately the same time, and reach the 
    same conclusion: the cache request is a miss!

    Therefore all of the user request threads will then move on to read
    the data from the SOR.  So, even though the application
    designer was careful to implement caching in the application, the 
    database is still subject to spikes of activity.  

    The thundering herd problem is made even worse when there are many 
    application servers to one database server, as the number of 
    simultaneous hits the database server may receive increases as a 
    function of the number of application servers deployed.

    <<Solution>>
    
    To solve the thundering herd problem, use the {{cache-as-sor}} pattern
    and Ehcache will automatically block all threads that are simultaneously 
    requesting a particular value and let one and only one thread through 
    to the database.  Once that thread has populated the cache, the 
    other threads will be allowed to read the cached value.

    Even better, when used in a cluster with Terracotta, Ehcache will 
    automatically coordinate access to the cache across the cluster, 
    and no matter how many application servers are deployed, still only 
    one user request will be serviced by the database on cache misses.

** data freshness problem

    When using a cache, "freshness" or data validity can be an issue.  This
    is particularly true when using a cache against an SOR like a database.

    Databases weren't built with caching outside of the database in mind,
    and therefore don't come with any default mechanism for notifying external
    processes when data has been updated or modified.

    <<Solution>>

    If up-to-date data is important for your application, consider these 
    strategies:

    * <<data expiration>>: use the eviction algorithms included with Ehcache
    along with the <<<timeToIdleSeconds>>> setting to enforce a maximum time
    for elements to live in the cache (forcing a re-load from the database
    or SOR).  You can read more about 
    {{{./cache_eviction_algorithms.html}cache eviction algorithms here}} and 
    the {{{./configuration.html}timeToIdle configuration setting here}}.

    * <<message bus>>: use an application to make all updates to the database.
    When updates are made, post a message onto a message queue with a key
    to the item that was updated.  All application instances can subscribe
    to the message bus and receive messages about data that is updated, and
    can synchronize their local copy of the data accordingly (for example
    by invalidating the cache entry for updated data)

    * <<triggers>>: Using a database trigger can accomplish a similar task
    as the message bus approach.  Use the database trigger to execute code
    that can publish a message to a message bus.  The advantage to this 
    approach is that all updates can be protected, but not all database
    triggers support full execution environments and it is often unadvisable
    to execute heavy-weight processing such as publishing messages on a 
    queue during a database trigger.

    []

** caching null values problem

    It is often application specific whether or not "null" values should    
    be cached.

    <<Solution>>

    Consider what is the best strategy for your application.  If data is    
    read frequently that does not exist and is the source of significant
    database load, consider caching a "negative hit" in the cache and 
    using a cache expiration setting described the {{data freshness 
    problem}} to periodically force a re-check of the value.

